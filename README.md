# OpenIssuesForML
record machine learning algorithm open issues and answers

|Author|Ximitiejiang|
|---|---
|E-mail|ximitiejiang@163.com

### Q_线性回归算法：如何计算l1,l2范数？两者的功能有什么区别？
1. l1范数
2. l2范数
3. 两者的区别：l1范数是lambda*(abs(distance))，l2范数是lambda*(distance**2)
4. 从可视化角度，或者从多约束问题理解，是两个等式的曲线相交点为方程的解。l1范数是矩形与曲线相交，
   所以解往往是0，也就是得到稀疏矩阵的解；而l2范数是圆形与曲线相交，所以解不会为0而是稠密解


### Q_线性回归算法：如何做非线性数据的回归？
1. 基于线性回归，做局部加权线性回归，可以达到对非线性数据的回归
2. 对变量进行多项式化，degree>=2就可以达到非线性回归
3. 采用CART决策树回归可以达到非线性回归


### Q_线性回归算法：线性回归中的截距项在模型中的作用是什么？(9/5机器学习算法与自然语言处理)
1. 噪声收集器？？？


### Q_逻辑回归算法：逻辑回归算法是否能做多分类问题，如果不能如何解决？
常规逻辑回归算法只能做二分类问题，因为他是把线性回归方程映射到（0，1），只能对0，1两类做分类。
1. 如果要实现多分类，需要采用OvsR(one vs. rest)，或者MvsM，即转化为二分类问题。
2. 还可以采用softmax算法实现多分类问题，softmax算法是对logistic regression算法的通用化。
评价方式就是：如果分类具有互斥性，采用softmax，如果有一定包容性，采用OvsO/MvsM 


### Q_逻辑回归算法：线性回归，逻辑回归，哪个更可能从大型矩阵乘法中获益？
1. 线性回归算法求解时theta = (xTx).I * xTy
2. 逻辑回归求解theta += alpha*xT*(yhat - h(xi))
可以看到逻辑回归主要依靠循环来逐步逼近theta，而线性回归可以通过一次矩阵计算求解
说明线性回归更可能从大型矩阵乘法中获益。


### Q_决策树算法：怎么计算信息熵和信息增益？一个骰子抛6次信息熵是多少？
信息熵Ent(D) = -sum(Pk * logPk)
所以对于一个骰子来说，Ent(D) = -6*(1/6 * log1/6) = 2.6 bit


### Q_决策树算法：决策树ID3算法为什么不能使用连续特征，如何解决？
1. 分类问题：可以用ID3或者CART
    * ID3决策树在做子数据集划分的时候采用的是按照最优特征的数值类型来划分的，
    有多少类就划分多少个子集，如果用连续特征，就会每个值划分一个数据集，导致模型基本没有泛化能力
    而CART决策树回归在做子数据集划分采用的是二分法，按照取值，每次都划分成2个子集，非常有利于连续特征的划分。
    而对于多类型离散特征的划分，需要对多类型特征排列组合多次划分，则相比于ID3需要更多次划分和计算量
2. 回归问题：只能用CART
    * 由于数据是连续的，没法计算信息熵或者gini指数，需要采用其他办法评估最优特征。可采用均方误差和来评估。


### Q_决策树算法：决策树预剪枝和后剪枝有什么区别？如何操作？
1. 预剪枝是在决策树生成过程中，对每个结点划分前进行估计，如果不能带来泛化能力提升，则停止划分并作为叶结点
2. 后剪枝是先训练出一个完整的树，然后自底向上对非叶结点考察，如果该子树用叶结点替换能带来泛化能力提高，则替换为叶结点
3. 评价泛化能力提升的算法方式：需要引入一个验证数据集，评价方式包括：
    * 的
    * 的


### Q_决策树算法：在分类问题上决策树算法是否支持多分类问题？
1. ID3决策树支持多分类，因为ID3做数据集切分是按照特征的取值类型分，多少类就分多少个子数据集
2. CART决策树也支持多分类


### Q_决策树算法：对于一个两层决策树和一个不带激活函数的两层神经网络，谁更强大？(9/5机器学习算法与自然语言处理)
1. 对于一个两层决策树(一层根节点，一层叶结点)，假定有3个叶子结点，能在二维数据上产生3段水平/垂直直线进行数据划分。
2. 对于一个不带激活函数的两层神经网络(一层隐藏层，一层输出层)，无论如何只能产生一条线性分隔
所以相对来说，两层决策树更强大些


### Q_决策树算法：决策树与神经网络都能进行非线性分类，但直观上为什么认为决策树比神经网络容易？(9/5机器学习算法与自然语言处理)
1. 决策树更可解释一些

### Q_朴素贝叶斯：如果有部分分类的概率等于0怎么办？比如一个数据实例：？？？
如果某个概率为0会导致计算分母为0的错误，需要通过拉普拉斯平滑，在分母增加一个N，分子增加一个1


### Q_朴素贝叶斯：对于小概率事件的分析预测，需要注意什么？
小概率？？？


### Q_adaBoost：为什么boost要用多个弱分类器，而不用强分类器？


### Q_adaBoost：偏差与方差的关系和区别，两者的权衡是什么？
1. 偏差：是预测值与实际值之间的误差
2. 方差：是模型预测值与均值的偏差，体现了单个模型与大部分模型(均值)的合群程度，也就是分散程度。
3. 通常是希望偏差和方差都越小越好，但越简单，偏差越大，方差较小；而模型越复杂，偏差下降了，而方差会上升。
4. boosting是串行模型，调整每次错误预测的权值，所以他会降低偏差，而对方差影响较小
   bagging是并行模型，采用放回采样机制，能够降低方差，但对偏差影响较小。
   不过bagging扩展后的随机森林能够同时降低方差和偏差。


### Q_降维算法：降维有什么好处，有哪些降维的方法？
1. 降维的好处：
    * 减少存储空间，加快计算速度，去除冗余特征，便于可视化
2. 降维的方法：
    * 去除缺失值过多的列
    * 低方差滤波：去除方差很小的特征列
    * 高相关滤波：把高相关特征去掉其中之一
    * 用随机森林评估特征重要性：去除不重要特征
    * 用PCA主成分分析：合并特征


### Q_降维算法：有哪些算法可以用来做特征提取，或者数据降维？(9/5机器学习算法与自然语言处理)
1. 理论上说，只要能够求解出特征参数进行比较，就能够作为降维工具
2. 在所有算法中，比如逻辑回归可以输出一组theta, 随机森林可以？？？，
   神经网络(结合自编码器)可以输出？？？


### Q_主成分分析算法：特征的相关性对主成分分析有何影响？(9/5机器学习算法与自然语言处理)
1. ？？？



### Q_特征处理：如何处理特征向量的缺失值？
特征向量缺失值处理方法有如下几种：
1. 缺失值较多：
    超过20%，则考虑直接舍弃该特征
2. 缺失值较少：
    * 用0值填充
        ```python
        train.isnull()
        train.fillna(0)
        ```
    * 用平均值填充
    * 用众数填充
    * 用插值法填充
    * 用随机森林法预测：
        > 将数据分为有值和缺失值2份，对有值的数据采用随机森林拟合，然后对有缺失值的数据进行预测，用预测的值来填充。


### Q_特征处理：如果在研究基因表达数据，发现样本只有几百个，而特征有几百万，说明为什么最小二乘模型不好？
因为？


### Q_特征处理：如何建立连续型特征与类别型特征的关系？


### Q_推荐算法：余弦相似和欧式距离有什么关系和区别？


### Q_深度学习的各种数据集有什么差别？各种深度学习网络的发展历史是如何的？
1. 数据集
    * MNIST是60000张的单个数字黑白图片；由Lecon维护
    * CIFAR是10种类别60000张彩色图片；
    * ImageNet是1500万张彩色多个类别高分辨率的图片；常用其中ILSVRC2012数据子集，
      由斯坦福大学李飞飞带头整理
    * 其他
2. 深度学习发展史：
    * 1998年，最早的卷积神经网络叫LeNet，是基于梯度学习的卷积神经网络，用于美国邮寄识别
    * 2012年，Hinton团队提出了Alexnet卷积神经网络，用到了ReLU激活函数，最大池化，Dropout以及
      GPU加速，但是为单隐藏层，获得当年的ImageNet比赛冠军
    * 2014年，Google提出了Inception-net，基于可反复堆叠的高效卷积神经网络，错误率为AlexNet一半
    * 2015年，微软团队提出了ResNet成功训练了152层深层次网络，把错误率降到3.46%













