# OpenIssuesForML
record machine learning algorithm open issues and answers

|Author|Ximitiejiang|
|---|---
|E-mail|ximitiejiang@163.com

### Q_线性回归算法：如何计算l1,l2范数？两者的功能有什么区别？
1. l1范数
2. l2范数
3. 两者的区别：l1范数是lambda*(abs(distance))，l2范数是lambda*(distance**2)


### Q_线性回归算法：如何做非线性数据的回归？
1. 基于线性回归，做局部加权线性回归，可以达到对非线性数据的回归
2. 对变量进行多项式化，degree>=2就可以达到非线性回归
3. 采用CART决策树回归可以达到非线性回归


### Q_逻辑回归算法：逻辑回归算法是否能做多分类问题，如果不能如何解决？
常规逻辑回归算法只能做二分类问题，因为他是把线性回归方程映射到（0，1），只能对0，1两类做分类。
1. 如果要实现多分类，需要采用OvsR(one vs. rest)，或者MvsM，即转化为二分类问题。
2. 还可以采用softmax算法实现多分类问题，softmax算法是对logistic regression算法的通用化。
评价方式就是：如果分类具有互斥性，采用softmax，如果有一定包容性，采用OvsO/MvsM 


### Q_决策树算法：怎么计算信息熵和信息增益？一个骰子抛6次信息熵是多少？
信息熵Ent(D) = -sum(Pk * logPk)
所以对于一个骰子来说，Ent(D) = -6*(1/6 * log1/6) = 2.6 bit


### Q_决策树算法：决策树ID3算法为什么不能使用连续特征，如何解决？
1. 分类问题：可以用ID3或者CART
    * ID3决策树在做子数据集划分的时候采用的是按照最优特征的数值类型来划分的，
    有多少类就划分多少个子集，如果用连续特征，就会每个值划分一个数据集，导致模型基本没有泛化能力
    而CART决策树回归在做子数据集划分采用的是二分法，按照取值，每次都划分成2个子集，非常有利于连续特征的划分。
    而对于多类型离散特征的划分，需要对多类型特征排列组合多次划分，则相比于ID3需要更多次划分和计算量
2. 回归问题：只能用CART
    * 由于数据是连续的，没法计算信息熵或者gini指数，需要采用其他办法评估最优特征。可采用均方误差和来评估。


### Q_朴素贝叶斯：如果有部分分类的概率等于0怎么办？比如一个数据实例：？？？
如果某个概率为0会导致计算分母为0的错误，需要通过拉普拉斯平滑，在分母增加一个N，分子增加一个1


### Q_adaBoost：为什么boost要用多个弱分类器，而不用强分类器？


### Q_adaBoost：偏差与方差的关系和区别，两者的权衡是什么？
1. 偏差：是预测值与实际值之间的误差
2. 方差：是模型预测值与均值的偏差，体现了单个模型与大部分模型(均值)的合群程度，也就是分散程度。
3. 通常是希望偏差和方差都越小越好，但越简单，偏差越大，方差较小；而模型越复杂，偏差下降了，而方差会上升。
4. boosting是串行模型，调整每次错误预测的权值，所以他会降低偏差，而对方差影响较小
   bagging是并行模型，采用放回采样机制，能够降低方差，但对偏差影响较小。
   不过bagging扩展后的随机森林能够同时降低方差和偏差。


### Q_降维算法：降维有什么好处，有哪些降维的方法？
1. 降维的好处：
    * 减少存储空间，加快计算速度，去除冗余特征，便于可视化
2. 降维的方法：
    * 去除缺失值过多的列
    * 低方差滤波：去除方差很小的特征列
    * 高相关滤波：把高相关特征去掉其中之一
    * 用随机森林评估特征重要性：去除不重要特征
    * 用PCA主成分分析：合并特征


### Q_数据处理：如何处理特征向量的缺失值？
特征向量缺失值处理方法有如下几种：
1. 缺失值较多：
    超过20%，则考虑直接舍弃该特征
2. 缺失值较少：
    * 用0值填充
        ```python
        train.isnull()
        train.fillna(0)
        ```
    * 用平均值填充
    * 用众数填充
    * 用插值法填充
    * 用随机森林法预测：
        > 将数据分为有值和缺失值2份，对有值的数据采用随机森林拟合，然后对有缺失值的数据进行预测，用预测的值来填充。


### Q_推荐算法：余弦相似和欧式距离有什么关系和区别？


